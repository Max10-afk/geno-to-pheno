fc_reg = keras.regularizers.L2(1e-10)
#l_relu = keras.activations.sigmoid()
class encoder(Model):
    def __init__(self, latent_dim, width = 1):
        super().__init__()
        self.latent_dim = latent_dim
        self.blocks = []
        for cur_block in range(width):
            self.blocks.append(
                [
                    layers.Dense(units=latent_dim, activation=None),
                    layers.LeakyReLU(negative_slope=0.5),
                    layers.Dense(units=latent_dim, activation=None),
                    layers.LeakyReLU(negative_slope=0.5)
                ]
            )
    def get_config(self):
        config = super().get_config() 
        config.update({
            'latent_dim': self.latent_dim,
            'blocks': [[keras.saving.serialize_keras_object(layer) for layer in block] for block in self.blocks]
        })
        return config

    @classmethod
    def from_config(cls, config):
        instance = cls(config['latent_dim'])
        
        # Deserialize each block's layers
        instance.blocks = []
        for block_config in config['blocks']:
            block_layers = []
            for layer_config in block_config:
                # Use the appropriate deserialization
                layer = keras.saving.deserialize_keras_object(layer_config)
                block_layers.append(layer)
            instance.blocks.append(block_layers)
        return instance

    def call(self, x):
        outputs = []
        x =  layers.Flatten()(x)
        for block in self.blocks:
            sub_x = x
            for layer in block:
                sub_x = layer(sub_x)
            outputs.append(sub_x)

        y = sum(outputs)

        return y


# Define the decoder model
fc_reg = keras.regularizers.L2(1e-5)
class decoder(Model):
    def __init__(self, latent_dim, depth = 1):
        super().__init__()
        self.latent_dim = latent_dim
        self.blocks = []
        for _ in range(3):
            # Create a block of layers
            block = []    
            # Add dense layers with Leaky ReLU activation after each
            for i in range(1, depth):
                block.append(layers.Dense(units=latent_dim * i))
                block.append(layers.LeakyReLU(negative_slope=0.5))
            # Add a final dense layer with fixed output units and kernel regularization
            block.append(layers.Dense(units=1144, kernel_regularizer=fc_reg))
            block.append(layers.LeakyReLU(negative_slope=0.5))  # Add Leaky ReLU after the final dense layer
            # Append the block to the blocks list
            self.blocks.append(block)

    def get_config(self):
        config = super().get_config()
        config.update({
            'latent_dim': self.latent_dim,
            'blocks': [[keras.saving.serialize_keras_object(layer) for layer in block] for block in self.blocks]
        })
        return config
    

    @classmethod
    def from_config(cls, config):
        instance = cls(config['latent_dim'])
        
        # Deserialize each block's layers
        instance.blocks = []
        for block_config in config['blocks']:
            block_layers = []
            for layer_config in block_config:
                # Use the appropriate deserialization
                layer = keras.saving.deserialize_keras_object(layer_config)
                block_layers.append(layer)
            instance.blocks.append(block_layers)
        return instance

    
    def call(self, x):

        outputs = []
        for block in self.blocks:
            sub_x = x
            for layer in block:
                sub_x = layer(sub_x)
            sub_x = tf.reshape(sub_x, (-1, 1, 1144))
            outputs.append(sub_x)

        y = tf.concat(outputs, axis=-2)
        y = layers.Softmax(axis=1)(y)
        return y


focal_loss = tf.keras.losses.CategoricalFocalCrossentropy(axis = 1,
                                                          from_logits=False, 
                                                          alpha = np.array([0.02, 0.92, 0.05])[..., np.newaxis,],
                                                          gamma=1)

focal_optimizer = optimizers.Lion(learning_rate=1e-4, use_ema = True, gradient_accumulation_steps = 3)# ema_overwrite_frequency = int(1e3))
focal_model = autoencoder(200, 5, 5)
focal_model.compile(optimizer=focal_optimizer, loss = focal_loss, run_eagerly=False,
                  metrics=[metrics.CategoricalAccuracy()])