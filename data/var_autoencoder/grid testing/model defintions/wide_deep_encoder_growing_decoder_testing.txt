# Define the Encoder model
fc_reg = keras.regularizers.L2(0.001)
l_relu = layers.LeakyReLU(negative_slope=0.5)
#l_relu = keras.activations.sigmoid()
class encoder(Model):
    def __init__(self, latent_dim, width = 1):
        super().__init__()
        self.latent_dim = latent_dim

        self.blocks = []
        for cur_block in range(3):
            self.blocks.append(
                [

                    layers.Flatten(),
                    
                    
                    
                ] +[layers.Dense(units=latent_dim * i, activation=l_relu)
                    for i in range(width, 0, -1)] +\
                [layers.Dense(units=latent_dim, activation=l_relu, kernel_regularizer=fc_reg)]
            )
    def get_config(self):
        config = super().get_config() 
        config.update({
            'latent_dim': self.latent_dim,
            # Conv layers are serializable by default
            'conv1_pre': self.conv1_pre.get_config(),
            'conv2_pre': self.conv2_pre.get_config(),
            'blocks': [[layer.get_config() for layer in block] for block in self.blocks]
        })
        return config

    @classmethod
    def from_config(cls, config):
        instance = cls(config['latent_dim'])
        instance.conv1_pre = layers.Conv1D.from_config(config['conv1_pre'])
        instance.conv2_pre = layers.Conv1D.from_config(config['conv2_pre'])
        instance.blocks = [[layers.deserialize(layer_config) for layer_config in block] for block in config['blocks']]
        return instance

    def call(self, x):
        outputs = []
        for block in self.blocks:
            sub_x = x
            for layer in block:
                sub_x = layer(sub_x)
            outputs.append(sub_x)

        y = sum(outputs)

        return y


# Define the decoder model
class decoder(Model):
    def __init__(self, latent_dim, depth = 1):
        super().__init__()
        self.latent_dim = latent_dim
        self.blocks = []
        for _ in range(3):
            self.blocks.append(
                    [layers.Dense(units=latent_dim * i, activation=l_relu) for i in range(1, depth)] +
                        [layers.Dense(units=1144, activation=l_relu, kernel_regularizer=fc_reg)]

            )
    def get_config(self):
        config = super().get_config()
        config.update({
            'latent_dim': self.latent_dim,
            'blocks': [[layer.get_config() for layer in block] for block in self.blocks]
        })
        return config

    @classmethod
    def from_config(cls, config):
        instance = cls(config['latent_dim'])
        instance.blocks = [[layers.deserialize(layer_config) for layer_config in block] for block in config['blocks']]
        return instance
    
    def call(self, x):

        outputs = []
        for block in self.blocks:
            sub_x = x
            for layer in block:
                sub_x = layer(sub_x)
            sub_x = tf.reshape(sub_x, (-1, 1, 1144))
            outputs.append(sub_x)

        y = tf.concat(outputs, axis=-2)
        y = layers.Softmax(axis=1)(y)
        return y
